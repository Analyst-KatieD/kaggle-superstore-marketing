{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e41fcd2-77a1-4e65-9ba6-79bb717a92ec",
   "metadata": {},
   "source": [
    "# Superstore Marketing Campaign Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2c27c6-cb34-45d7-937e-e90664d387c4",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "\n",
    "For this project the client is preparing for their year-end sale. They would like to launch a new gold membership offer. This gold membership gives a 20% discount on all purchases for the cost of \\\\$499 instead of the normal pricing of \\\\$999. It will be valid only for existing customers and the campaign through phone calls is currently being planned for them. The management feels that the best way to reduce the cost of the campaign is to make a predictive model which will classify customers who might purchase the offer. \n",
    "\n",
    "**Project Goals:**\n",
    "* Predict the likelihood of a customer giving a positive response to the latest marketing campaign.\n",
    "* Identify the features that contribute the most to the likelihood of a positive response to the campaign.\n",
    "* List of customers that are the most likely to purchase to start an outbound calling campaign.\n",
    "\n",
    "Kaggle Project: https://www.kaggle.com/datasets/ahsan81/superstore-marketing-campaign-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f5176-2f66-4612-b23c-9a23c75a0050",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "---\n",
    "\n",
    "1. [Exploratory Analysis](#section-1-exploratory-analysis)\n",
    "2. [Data Cleaning](#section-2-data-cleaning)\n",
    "3. Feature Prep\n",
    "4. Data Splitting\n",
    "5. Create Baseline Model\n",
    "6. Optimize Model & Final Model Summary\n",
    "7. Export Data for Calling Campaign\n",
    "8. Executive Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13eedb-7ede-4b11-9760-ddcd5da04627",
   "metadata": {},
   "source": [
    "## Section 1: Exploratory Analysis\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*   Load dataset & inspect shape, datatypes, sample rows\n",
    "*   Check target variable distribution (class balance)\n",
    "*   Note data sources, time periods, and potential leakage risks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15559165-8b91-460e-956c-ee82257133b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Dt_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>...</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>MntGoldProds</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>Response</th>\n",
       "      <th>Complain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1501</td>\n",
       "      <td>1982</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>160803.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-04-08</td>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8475</td>\n",
       "      <td>1973</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>157243.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>98</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8720</td>\n",
       "      <td>1978</td>\n",
       "      <td>2n Cycle</td>\n",
       "      <td>Together</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-08</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5555</td>\n",
       "      <td>1975</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>153924.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-02</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11181</td>\n",
       "      <td>1949</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>156924.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-29</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Year_Birth   Education Marital_Status    Income  Kidhome  Teenhome  \\\n",
       "0   1501        1982         PhD        Married  160803.0        0         0   \n",
       "1   8475        1973         PhD        Married  157243.0        0         1   \n",
       "2   8720        1978    2n Cycle       Together       NaN        0         0   \n",
       "3   5555        1975  Graduation       Divorced  153924.0        0         0   \n",
       "4  11181        1949         PhD        Married  156924.0        0         0   \n",
       "\n",
       "  Dt_Customer  Recency  MntWines  ...  MntFishProducts  MntSweetProducts  \\\n",
       "0  2012-04-08       21        55  ...               17                 3   \n",
       "1  2014-01-03       98        20  ...                1                 2   \n",
       "2  2012-12-08       53        32  ...               12                 4   \n",
       "3  2014-07-02       81         1  ...                1                 1   \n",
       "4  2013-08-29       85         2  ...                1                 1   \n",
       "\n",
       "   MntGoldProds  NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "0             4                 15                0                   28   \n",
       "1             1                 15                0                   22   \n",
       "2            22                  0                0                    0   \n",
       "3             1                  0                0                    0   \n",
       "4             1                  0                0                    0   \n",
       "\n",
       "   NumStorePurchases  NumWebVisitsMonth  Response  Complain  \n",
       "0                  1                  0         0         0  \n",
       "1                  0                  0         0         0  \n",
       "2                  1                  0         0         0  \n",
       "3                  0                  0         0         0  \n",
       "4                  0                  0         0         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"C:\\Users\\katie\\Documents\\Portfolio-Projects\\kaggle-superstore-marketing\\superstore_data.csv.csv\"\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to confirm it loaded correctly\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42f3a71-5b8a-486d-9d55-d120af3d6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #more numeric formulas\n",
    "import matplotlib.pyplot as plt #for nice histograms\n",
    "import seaborn as sns #for boxplots and heat map\n",
    "from statsmodels.formula.api import logit #to build the regression model\n",
    "import scipy.stats as stats #to calculate z-score\n",
    "import missingno as msno #to visualize missing values\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm #for residual plots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99a70bf-48fc-4cc5-9e1d-c40dabb1f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2240 entries, 0 to 2239\n",
      "Data columns (total 22 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Id                   2240 non-null   int64  \n",
      " 1   Year_Birth           2240 non-null   int64  \n",
      " 2   Education            2240 non-null   object \n",
      " 3   Marital_Status       2240 non-null   object \n",
      " 4   Income               2216 non-null   float64\n",
      " 5   Kidhome              2240 non-null   int64  \n",
      " 6   Teenhome             2240 non-null   int64  \n",
      " 7   Dt_Customer          2240 non-null   object \n",
      " 8   Recency              2240 non-null   int64  \n",
      " 9   MntWines             2240 non-null   int64  \n",
      " 10  MntFruits            2240 non-null   int64  \n",
      " 11  MntMeatProducts      2240 non-null   int64  \n",
      " 12  MntFishProducts      2240 non-null   int64  \n",
      " 13  MntSweetProducts     2240 non-null   int64  \n",
      " 14  MntGoldProds         2240 non-null   int64  \n",
      " 15  NumDealsPurchases    2240 non-null   int64  \n",
      " 16  NumWebPurchases      2240 non-null   int64  \n",
      " 17  NumCatalogPurchases  2240 non-null   int64  \n",
      " 18  NumStorePurchases    2240 non-null   int64  \n",
      " 19  NumWebVisitsMonth    2240 non-null   int64  \n",
      " 20  Response             2240 non-null   int64  \n",
      " 21  Complain             2240 non-null   int64  \n",
      "dtypes: float64(1), int64(18), object(3)\n",
      "memory usage: 385.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f44e00-0fac-4607-9b5f-a01fa24dbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot using catplot\n",
    "sns.catplot(x='Education', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf30ab1-5412-407f-8cc1-9cc8d31728fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot using catplot\n",
    "sns.catplot(x='Marital_Status', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Marital_Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dcf11-8623-4773-b5a6-bca34851111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the object column to datetime\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%m/%d/%Y')\n",
    "\n",
    "# Check the result\n",
    "print(df['Dt_Customer'].dtype)\n",
    "df['Dt_Customer'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900bea6-71a0-4434-8692-9d41f166349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: YYYY-MM\n",
    "df['Join_YearMonth'] = df['Dt_Customer'].dt.to_period('M')\n",
    "\n",
    "# Count customers per year-month\n",
    "join_counts = df['Join_YearMonth'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "join_counts.plot(kind='bar', edgecolor='k')\n",
    "plt.title('Number of Customers Joined per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80969d-0822-4538-9fcd-af377f4edc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0488ec-fad4-4600-afd0-99a38e69bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in df.columns if col not in ['Response', 'Marital_Status', 'Education', 'Id', 'Complain','Dt_Customer', 'Join_YearMonth' ]]\n",
    "\n",
    "for col in columns:\n",
    "    # --- Chart 1: Histogram ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    values, bins, bars = plt.hist(df[col], bins=10, edgecolor='white')\n",
    "    plt.title(f'{col}')\n",
    "    plt.bar_label(bars)\n",
    "    plt.margins(x=0.01, y=0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6dff1-5021-4379-81e8-0a359a29d5a2",
   "metadata": {},
   "source": [
    "### Section 1 Overview:\n",
    "In the 2 categorical text fields there are weird options that likely should be dropped from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640eb4f2-38c4-4909-a45f-4ae6ac036f2a",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning\n",
    "---\n",
    "\n",
    "\n",
    "*   Remove duplicates & invalid rows\n",
    "*   Correct data types (numeric, categorical, datetime)\n",
    "*   Handle missing values\n",
    "    *   Compute % missing per column\n",
    "    *   Decide drop vs. impute\n",
    "    *   Add ‚Äúmissing flag‚Äù columns if useful\n",
    "*   Detect & treat outliers (IQR / z-score / domain rules)\n",
    "*   Standardize categorical labels (case, spacing, typos)\n",
    "*   Combine rare categories (‚ÄúOther‚Äù)\n",
    "*   Check and fix inconsistent encodings (Yes/No vs. 1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45735216-d351-4df5-b566-8d59bdac6488",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#  Check for Duplicate Rows\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#  Check for Duplicate Rows\n",
    "df.duplicated(['Id']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515b28b-0150-4b58-9d12-1f64bd8e01f4",
   "metadata": {},
   "source": [
    "No duplicate entries to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b84118-4069-42fa-8144-104b8107c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Categorical strings to update the weird responses to an unknown option\n",
    "# Education:  2n Cycle >> Unknown\n",
    "# Marital_Status: Yolo, Absurd >> Unknown,  Alone >> Single, Together >> Married\n",
    "# df['col'] = df['col'].replace({'ny': 'new york', 'n.y.': 'new york'})\n",
    "\n",
    "df['Education'] = df['Education'].replace({'2n Cycle': 'Unknown'})\n",
    "df['Marital_Status'] = df['Marital_Status'].replace({'YOLO':'Unknown', 'Absurd':'Unknown', 'Alone':'Single', 'Together':'Married'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15f9b4-2869-4384-a6b7-30aff29ef3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Education', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2b1e5-c95a-4950-b263-786603d86fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Marital_Status', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Marital_Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676fdb4-d793-49b7-8174-4f552da25a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a matrix plot to review for missing values\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3c274-52f1-4ed6-a88e-8d5b10496647",
   "metadata": {},
   "source": [
    "Income is only field with missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d149ee-a73a-4642-9250-3569fd9453cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df['Income'].dropna(), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48f2d9-c059-41a0-812b-e0fc18de681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "values, bins, bars = plt.hist(df['Income'], bins=10, edgecolor='white', range =(0,175000))\n",
    "plt.title('Income')\n",
    "plt.margins(x=0.1, y=0.1)\n",
    "plt.bar_label(bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4427f-b2f0-44ff-a241-9f8fbfc4674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4808cd5-439d-45a9-8ffa-14b594cebc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income = float(np.median(df['Income'].dropna()))\n",
    "median_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f9d01-a0cd-4864-8838-288db618faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Income'] = df['Income'].fillna(median_income)\n",
    "\n",
    "df['Income'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe0fdd-8e26-4be8-bf6d-e31b6474ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88e617-b659-426d-8b88-072967213c93",
   "metadata": {},
   "source": [
    "#### Combine all of the 'Spending' Columns into a total spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254caaec-d67c-4891-a20d-d70bf16a9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_cols = ['MntWines', 'MntMeatProducts', 'MntFruits', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "df['Total_Spent'] = df[spending_cols].sum(axis=1)\n",
    "print(df[['Total_Spent', 'MntWines', 'MntMeatProducts', 'MntFruits', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c899a7-e3fb-4d20-b115-af61ce154373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['MntWines', 'MntMeatProducts', 'MntFruits', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff3036-dcf9-431d-a28a-ce1ebc349e92",
   "metadata": {},
   "source": [
    "#### Combine all the purchases into 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff1e05-a4bc-4680-999c-90f889d93688",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_cols = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "df['Total_Purchases'] = df[purchases_cols].sum(axis=1)\n",
    "print(df[['Total_Purchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c678330-5364-40bb-aa0e-fec1d0766203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases'], inplace=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b6aa5-0ac6-41fd-beca-339382b220f7",
   "metadata": {},
   "source": [
    "#### Check and fix outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbbb8d-1f2d-4c64-bd1a-ac6083e66aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the df before outlier detection and replacement \n",
    "df_original = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528aefa-8ad3-4957-a19d-e860774e5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to ignore\n",
    "ignore_cols = ['Response', 'Marital_Status', 'Education', 'Id', 'Complain', 'Kidhome', 'Teenhome']\n",
    "\n",
    "# Numeric columns to process\n",
    "num_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col not in ignore_cols]\n",
    "\n",
    "# Loop through each numeric column\n",
    "for col in num_cols:\n",
    "    # Copy column for \"before\" boxplot\n",
    "    before = df[col].copy()\n",
    "    \n",
    "    # Calculate IQR bounds\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Define caps at 5th and 95th percentiles\n",
    "    low_cap = df[col].quantile(0.05)\n",
    "    high_cap = df[col].quantile(0.95)\n",
    "    \n",
    "    # Count outliers before replacement\n",
    "    num_outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    \n",
    "    # Replace outliers\n",
    "    df[col] = df[col].apply(lambda x: low_cap if x < lower else high_cap if x > upper else x)\n",
    "    \n",
    "    # Copy column for \"after\" boxplot\n",
    "    after = df[col]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"{col}: {num_outliers} outliers replaced\")\n",
    "    \n",
    "    # Plot side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axes[0].boxplot(before)\n",
    "    axes[0].set_title(f\"{col} - Before Outlier Handling\")\n",
    "    \n",
    "    axes[1].boxplot(after)\n",
    "    axes[1].set_title(f\"{col} - After Outlier Handling\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8799eb-a34a-4c54-ad41-8926d98d9104",
   "metadata": {},
   "source": [
    "## 3. Feature Preparation\n",
    "\n",
    "*   Transform skewed numeric variables (log/sqrt if needed)\n",
    "*   Create derived features (ratios, bins, interactions if justified)\n",
    "*   Encode categorical variables\n",
    "    *   One-hot for nominal\n",
    "    *   Ordinal encoding for ordered categories\n",
    "*   Scale numeric features (standard or robust scaling)\n",
    "*   Check correlations & VIF for multicollinearity\n",
    "*   Drop or combine highly correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70be6d7-e983-4d20-a29b-c211447cba6a",
   "metadata": {},
   "source": [
    "### Transform Numeric Variables\n",
    "\n",
    "*   Evaluate for skew\n",
    "*   Evaluate for scale\n",
    "*   Transform both as needed \n",
    "*   Report changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9681e-b7fe-4f41-98c8-6ecc2749d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the df before transformed skewed variables\n",
    "df_outlier = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff937793-598f-483c-be39-a6aa5beb4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "def preprocess_numeric(df, ignore_cols=None, skew_threshold=1.0, transformation='log', scaling='standard'):\n",
    "    \"\"\"\n",
    "    Handle skewed numeric features and scale them for modeling.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - ignore_cols: list of columns to skip (e.g., target, booleans)\n",
    "    - skew_threshold: absolute skewness above which to transform\n",
    "    - transformation: 'log', 'sqrt', or 'boxcox'\n",
    "    - scaling: 'standard' or 'robust'\n",
    "\n",
    "    Returns:\n",
    "    - df_scaled: DataFrame with transformed and scaled numeric features\n",
    "    \"\"\"\n",
    "    if ignore_cols is None:\n",
    "        ignore_cols = []\n",
    "\n",
    "    # Select numeric columns to process\n",
    "    num_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col not in ignore_cols]\n",
    "\n",
    "    # Initialize scaler\n",
    "    if scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaling must be 'standard' or 'robust'\")\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    for col in num_cols:\n",
    "        data_before = df_scaled[col].copy()\n",
    "        skewness_before = data_before.skew()\n",
    "        transformation_applied = \"None\"\n",
    "\n",
    "        # Transform if skewed\n",
    "        if abs(skewness_before) > skew_threshold:\n",
    "            if transformation == 'log':\n",
    "                if (data_before <= 0).any():\n",
    "                    df_scaled[col] = np.log1p(data_before)  # log(1+x) handles zeros\n",
    "                    transformation_applied = \"log1p\"\n",
    "                else:\n",
    "                    df_scaled[col] = np.log(data_before)\n",
    "                    transformation_applied = \"log\"\n",
    "            elif transformation == 'sqrt':\n",
    "                df_scaled[col] = np.sqrt(data_before.clip(lower=0))\n",
    "                transformation_applied = \"sqrt\"\n",
    "            elif transformation == 'boxcox':\n",
    "                if (data_before <= 0).any():\n",
    "                    transformation_applied = \"Box-Cox not applied (non-positive values)\"\n",
    "                else:\n",
    "                    df_scaled[col], _ = stats.boxcox(data_before)\n",
    "                    transformation_applied = \"Box-Cox\"\n",
    "        else:\n",
    "            df_scaled[col] = data_before  # no transformation\n",
    "\n",
    "        # Scale the column\n",
    "        df_scaled[col] = scaler.fit_transform(df_scaled[[col]])\n",
    "\n",
    "        # Plot before and after histograms\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(data_before, bins=30, edgecolor='k', alpha=0.7)\n",
    "        plt.title(f\"{col} - Before\\nSkewness: {skewness_before:.2f}\")\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(df_scaled[col], bins=30, edgecolor='k', alpha=0.7)\n",
    "        plt.title(f\"{col} - After ({transformation_applied}, {scaling} scaled)\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{col}: Transformation -> {transformation_applied}, Scaling -> {scaling}\")\n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095b4f7-4c74-4a10-bf9f-fc28b6117958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = preprocess_numeric(df, ignore_cols=ignore_cols, transformation='log', scaling='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a1d7b-5c8a-4979-a9d0-13f1649d1220",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315aa78b-5e84-4551-930d-6ff850948064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_preprocessed.copy() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78797d37-12c7-44ca-b574-ebbd7732f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dt_customer field to bins so the logistic model is able to use this data effectively\n",
    "df['Customer_Tenure_Bin'] = pd.cut(\n",
    "    df['Dt_Customer'],\n",
    "    bins=pd.to_datetime(['2012-01-01','2012-06-30','2012-12-31',\n",
    "                         '2013-06-30','2013-12-31','2014-06-30','2014-12-31']),\n",
    "    labels=['H1 2012','H2 2012','H1 2013','H2 2013','H1 2014','H2 2014']\n",
    ")\n",
    "\n",
    "df['Customer_Tenure_Bin'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d6d09-8c10-4e2e-9f3a-2d1e7a007cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Customer_Tenure_Bin', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Customer_Tenure_Bin')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb7f03-c228-437f-9145-5fc6364b6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Dt_Customer', 'Join_YearMonth'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e3ed7-2b55-4428-8325-596284c22bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the field Year_Birth to bins\n",
    "# Compute age as of 2015\n",
    "df['Age'] = 2015 - df['Year_Birth']\n",
    "\n",
    "# Define age bins and labels\n",
    "bins = [0, 24, 34, 44, 54, 64, float('inf')]  # last bin = 65+\n",
    "labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "\n",
    "# Bin the ages\n",
    "df['Age_Bin'] = pd.cut(df['Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Verify\n",
    "print(df[['Year_Birth', 'Age', 'Age_Bin']].head(10))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec2bcc-c4aa-4799-bd9c-efcf18006bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Age_Bin', kind='count', data=df, height=4, aspect=2)\n",
    "plt.title('Count of Observations per Option')\n",
    "plt.xlabel('Age_Bin')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d972e-4382-4bf6-b4ca-5c51dfe9cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Year_Birth', 'Age'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6471b-7dde-472d-b3d1-da4166f6462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect categorical columns automatically\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns detected for encoding: {cat_cols}\")\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Shape before encoding: {df.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e44ba8-6366-4b0b-bc45-20c783d5cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['Response'] = df_encoded['Response'].astype(bool)\n",
    "df_encoded['Complain'] = df_encoded['Complain'].astype(bool)\n",
    "\n",
    "# Verify the changes\n",
    "print(df_encoded[['Response', 'Complain']].dtypes)\n",
    "df_encoded[['Response', 'Complain']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab0f56-2a3a-4ab1-aebc-4317e90ed6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364551f-054b-4b6e-9332-13ef77b71d20",
   "metadata": {},
   "source": [
    "### Correlations & Multicollinearity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa00fb-2bf7-42d6-9819-ab5a4f19b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def correlation_and_vif(df, target_col, vif_threshold=5.0):\n",
    "    \"\"\"\n",
    "    Computes correlation with target and VIF for numeric features.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - target_col: name of target column\n",
    "    - vif_threshold: threshold to flag high multicollinearity\n",
    "    \n",
    "    Returns:\n",
    "    - report_df: DataFrame with feature, correlation with target, VIF, and multicollinearity flag\n",
    "    \"\"\"\n",
    "    # 1. Separate features from target\n",
    "    feature_cols = [col for col in df.columns.drop(target_col) if df[col].var() > 0]\n",
    "    X = df[feature_cols].copy().astype(float)\n",
    "\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # 2. Correlation with target\n",
    "    corr_with_target = X.corrwith(y)\n",
    "    \n",
    "    # 3. Compute VIF\n",
    "    vif_values = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    # 4. Compile report\n",
    "    report_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Correlation_with_Target': corr_with_target,\n",
    "        'VIF': vif_values\n",
    "    })\n",
    "    \n",
    "    # Flag high multicollinearity\n",
    "    report_df['High_VIF'] = report_df['VIF'] > vif_threshold\n",
    "    \n",
    "    # Sort by absolute correlation descending\n",
    "    report_df = report_df.reindex(report_df['Correlation_with_Target'].abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"Correlation and VIF Report (sorted by |correlation|):\\n\")\n",
    "    print(report_df.to_string(index=False))\n",
    "    print(f\"\\nFeatures with VIF > {vif_threshold} are flagged as 'High_VIF=True'.\")\n",
    "    \n",
    "    return report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e71c83-c2be-4dbb-950f-ac3cbc9fc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_encoded is your processed DataFrame\n",
    "report = correlation_and_vif(df_encoded, target_col='Response', vif_threshold=5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7ad1f-34c9-4661-98bb-a9e11c1b4e83",
   "metadata": {},
   "source": [
    "Column Income will be dropped to help with multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95224e-76e6-43dd-be9c-877c3d84e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Income column\n",
    "df_encoded.drop(columns=['Income'], inplace=True)\n",
    "\n",
    "# Optional: re-run correlation and VIF to confirm\n",
    "report2 = correlation_and_vif(df_encoded, target_col='Response', vif_threshold=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37366ae4-7308-4609-a9c0-10e33206fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931e915-2ffa-4f9b-8bec-d8dee9ee1bce",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "   * Split dataset into train/test before target-informed steps\n",
    "   * Use stratified split for classification\n",
    "   * Optionally create validation set for tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e35c1-b6c3-4a05-a140-49004af86e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: drop the target\n",
    "X = df_encoded.drop(columns=['Response'])\n",
    "\n",
    "# Target\n",
    "y = df_encoded['Response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ec598-e309-44b2-b96f-a264a7322f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train / 20% test (you can adjust)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,  # ensures reproducibility\n",
    "    stratify=y         # keeps the same proportion of 0/1 in train and test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d566c-f317-453c-8493-68d6cc3b0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"y_test distribution:\\n\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a70a8bbf-9ac6-427a-99b8-d7f17b06bf62",
   "metadata": {},
   "source": [
    "## 5. Baseline & Imbalance Handling\n",
    "   * Fit baseline (intercept-only or simple) logistic model\n",
    "   * Examine baseline metrics\n",
    "   * Address imbalance\n",
    "       * ‚ÄÉ‚ÄÉClass weights\n",
    "       * ‚ÄÉ‚ÄÉOversampling / undersampling / SMOTE (training only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d5408-df1b-4c63-8e76-f6a6d2e47f38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Version 1 - Commented Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70f235-c9d1-4b48-872f-cb81fcb82a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_cols = ['Recency', 'NumDealsPurchases', 'NumWebVisitsMonth', 'Total_Spent', 'Total_Purchases']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "# X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c22d93-6b11-43f0-b160-27766c109fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# # Baseline model (use default solver)\n",
    "# baseline_model = LogisticRegression(\n",
    "#     random_state=42,\n",
    "#     max_iter=5000  # ensure convergence\n",
    "# )\n",
    "\n",
    "# # Fit on training data\n",
    "# baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on test data\n",
    "# y_pred = baseline_model.predict(X_test)\n",
    "# y_proba = baseline_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4e2e8-73a2-43e1-b0b7-4e4ba8b50bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Target distribution in training set:\\n\", y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313c8b0-c891-458a-b7a5-e0fbbb961a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic regression with balanced class weights\n",
    "# model_balanced = LogisticRegression(\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42,\n",
    "#     max_iter=5000\n",
    "# )\n",
    "# model_balanced.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe29300-773a-49eb-9bd0-a8cf760d49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict\n",
    "# y_pred_bal = model_balanced.predict(X_test)\n",
    "# y_proba_bal = model_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"Accuracy (balanced):\", accuracy_score(y_test, y_pred_bal))\n",
    "# print(\"ROC-AUC (balanced):\", roc_auc_score(y_test, y_proba_bal))\n",
    "# print(\"Confusion Matrix (balanced):\\n\", confusion_matrix(y_test, y_pred_bal))\n",
    "# print(\"\\nClassification Report (balanced):\\n\", classification_report(y_test, y_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2af02-9680-4284-acc6-e48fd8c0d0ef",
   "metadata": {},
   "source": [
    "#### Version 2 - Baseline and Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e49f07-c2aa-4549-b468-434c25aa5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Define numeric columns\n",
    "numeric_cols = ['Recency', 'NumDealsPurchases', 'NumWebVisitsMonth', 'Total_Spent', 'Total_Purchases']\n",
    "\n",
    "# -----------------------------\n",
    "# Define pipelines for baseline and tuned models\n",
    "# Baseline: default L2 logistic regression\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=5000))\n",
    "])\n",
    "\n",
    "# Tuned logistic regression pipeline (GridSearchCV)\n",
    "param_grid = {\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'logreg__solver': ['saga'],            # saga supports elasticnet and L1\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],       # regularization strength\n",
    "    'logreg__class_weight': [None, 'balanced'],\n",
    "    'logreg__l1_ratio': [0.0, 0.5, 1.0]    # only used if penalty='elasticnet'\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(max_iter=5000, random_state=42))\n",
    "    ]),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Fit baseline model\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "y_pred_base = baseline_pipeline.predict(X_test)\n",
    "y_proba_base = baseline_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "baseline_metrics = {\n",
    "    'Model': 'Baseline',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_base),\n",
    "    'Precision': precision_score(y_test, y_pred_base),\n",
    "    'Recall': recall_score(y_test, y_pred_base),\n",
    "    'F1': f1_score(y_test, y_pred_base),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_proba_base)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Fit GridSearch (tuned) model\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "y_proba_tuned = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "tuned_metrics = {\n",
    "    'Model': 'Tuned',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned),\n",
    "    'Precision': precision_score(y_test, y_pred_tuned),\n",
    "    'Recall': recall_score(y_test, y_pred_tuned),\n",
    "    'F1': f1_score(y_test, y_pred_tuned),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_proba_tuned)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Compile metrics into DataFrame\n",
    "metrics_df = pd.DataFrame([baseline_metrics, tuned_metrics])\n",
    "metrics_df = metrics_df.sort_values(by='ROC-AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"üèÜ Model Comparison by ROC-AUC:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Optional: print confusion matrix & classification report for best model\n",
    "print(\"\\nConfusion Matrix (Best Model):\\n\", confusion_matrix(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report (Best Model):\\n\", classification_report(y_test, y_pred_tuned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6defe7-7b08-459f-b0c5-8780c1a8ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# -----------------------------\n",
    "# ROC curve for Baseline\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_proba_base)\n",
    "roc_auc_base = auc(fpr_base, tpr_base)\n",
    "\n",
    "# ROC curve for Tuned\n",
    "fpr_tuned, tpr_tuned, _ = roc_curve(y_test, y_proba_tuned)\n",
    "roc_auc_tuned = auc(fpr_tuned, tpr_tuned)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot side-by-side ROC curves\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.plot(fpr_base, tpr_base, color='blue', lw=2, label=f'Baseline ROC-AUC = {roc_auc_base:.3f}')\n",
    "plt.plot(fpr_tuned, tpr_tuned, color='green', lw=2, label=f'Tuned ROC-AUC = {roc_auc_tuned:.3f}')\n",
    "plt.plot([0,1], [0,1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison: Baseline vs. Tuned Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Summary metrics table\n",
    "metrics_df = pd.DataFrame([baseline_metrics, tuned_metrics])\n",
    "metrics_df = metrics_df.sort_values(by='ROC-AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"üèÜ Best Performing Model:\")\n",
    "print(f\"Model: {metrics_df.loc[0, 'Model']}\")\n",
    "print(f\"ROC-AUC Score: {metrics_df.loc[0, 'ROC-AUC']:.3f}\\n\")\n",
    "\n",
    "print(\"üîπ Model Comparison Table:\")\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"\\nüí¨ Interpretation:\")\n",
    "print(\"- ROC-AUC is the recommended metric for imbalanced data.\")\n",
    "print(\"- Baseline prioritizes precision (fewer false positives).\")\n",
    "print(\"- Tuned model may improve recall (fewer missed positive customers).\")\n",
    "print(\"- Choose the model depending on business need: prioritize catching positives (recall) vs. avoiding false alarms (precision).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe63ccc-5fba-4e5f-bc6f-09e52c77f281",
   "metadata": {},
   "source": [
    "## Prep Export for calling promo team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716d554-278c-437d-afc6-3635e7f553aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tuned_model exists\n",
    "tuned_model = grid_search.best_estimator_  # if you used grid search\n",
    "\n",
    "# Copy the original dataframe\n",
    "df_customers = df_encoded.copy()\n",
    "\n",
    "# Predict probabilities using the tuned model\n",
    "df_customers['Predicted_Probability'] = tuned_model.predict_proba(df_encoded.drop(columns=['Response']))[:, 1]\n",
    "\n",
    "# Set threshold for labeling as \"likely to buy\"\n",
    "threshold = 0.5\n",
    "df_customers['Predicted_Label'] = df_customers['Predicted_Probability'] >= threshold\n",
    "\n",
    "# Optional: rank customers from most to least likely\n",
    "df_customers_sorted = df_customers.sort_values(by='Predicted_Probability', ascending=False)\n",
    "\n",
    "# Display top 10 customers\n",
    "df_customers_sorted[['Id', 'Predicted_Probability', 'Predicted_Label']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49111bba-ae6d-4598-89df-889b67426126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "# df_customers_sorted.to_csv(\"customer_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59f10c-cb15-4550-a582-b70a6ad24508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "# 1Ô∏è‚É£ Histogram of predicted probabilities\n",
    "sns.histplot(df_customers['Predicted_Probability'], bins=30, kde=True, color='skyblue', ax=axes[0])\n",
    "axes[0].set_title(\"Distribution of Predicted Probabilities\")\n",
    "axes[0].set_xlabel(\"Predicted Probability of Buying\")\n",
    "axes[0].set_ylabel(\"Number of Customers\")\n",
    "\n",
    "# 2Ô∏è‚É£ Count of predicted labels\n",
    "sns.countplot(x='Predicted_Label', data=df_customers, palette='pastel', ax=axes[1])\n",
    "axes[1].set_title(\"Count of Predicted Labels\")\n",
    "axes[1].set_xlabel(\"Predicted Label (Likely to Buy)\")\n",
    "axes[1].set_ylabel(\"Number of Customers\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113b6ed-d28c-4e7c-b1f2-4850e7d7b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "git config --global user.email \"katie.delvalle@gmail.com\"\n",
    "git config --global user.name \"Katie DelValle\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-python310env]",
   "language": "python",
   "name": "conda-env-anaconda3-python310env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
